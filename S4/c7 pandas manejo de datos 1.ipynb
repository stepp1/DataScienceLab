{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA6202: Laboratorio de Ciencia de Datos\n",
    "\n",
    "**Profesor: Nicolás Caro**\n",
    "\n",
    "**20/04/2020 - C7 S4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manejo de Datos en Python\n",
    "\n",
    "El manejo adecuado de datos es (naturalmente) un aspecto fundamental en ciencia de datos. Como se ha visto, `NumPy` permite un manejo básico de datos a través de sus operaciones sobre arreglos. No obstante, las tareas referentes al manejo de datos requieren habitualmente de funcionalidades más especificas. \n",
    "\n",
    "En este contexto nace la librería `Pandas`. Esta se concibió como una extensión de `NumPy` basada en software libre y dirigida específicamente a la manipulación y análisis de datos en Python. \n",
    "\n",
    "`Pandas` provee estructuras y operaciones para el trabajo de tablas numéricas y series de tiempo, es estándar en aplicaciones de ciencia de datos y su manejo es el eje pricipal de esta cátedra. \n",
    "\n",
    "### Elementos básicos de Pandas\n",
    "\n",
    "`Pandas` usa en conjunto con librerías de computación numérica (como `Numpy` y `SciPy`), librerías de visualización (como `matplotlib` y `seaborn`), librerías de analítica (como `statsmodels` y `scikit-learn`), entre otras.\n",
    "\n",
    "El manejo de datos con `Pandas` toma los elementos de `Numpy` en cuanto a computación basada en arreglos y los expande al manejo de datos heterogéneos.\n",
    "\n",
    "Como convención, `Pandas` se importa de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las clases que forman el núcleo de pandas corresponden a ```Series``` y ```DataFrame```.\n",
    "\n",
    "### Series\n",
    "\n",
    "Los objetos tipo ```Series``` consisten en un arreglo unidimensional , contenedor de una sucesión de valores (u objetos). A tal objeto, se asocia un índice o `index`, el cual consiste en un nuevo arreglo, con etiquetas o *id's* vinculadas a los datos.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se define una serie sencilla a partir de una lista. Observe que se hace conversion de tipo de datos ```list``` $\\to$  ```pd.Series``` de manera nativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie = pd.Series([1,9,7, -5, 3,10])\n",
    "serie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La operación anterior imprime en pantalla los valores de la serie y a su izquierda su índice correspondiente (por medio de su representación ```__repr__```). Es posible especificar el formato sobre la indexación, el valor por defecto es el anterior y consiste en valores enteros entre `0`  y `N-1`, donde `N` es la cantidad de valores en la serie.\n",
    "\n",
    "Para especificar los índices en cada elemento de la serie, se puede acceder al atributo `index` de esta y modificarlo o especificar la configuración deseada al declarar la serie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar atributo \"index\" una vez definida la serie\n",
    "serie.index = range(1,7)\n",
    "\n",
    "print('serie, indice como rango: \\n')\n",
    "print(serie, '\\n')\n",
    "\n",
    "serie.index = list('abcdef')\n",
    "print('serie, indice como lista: \\n')\n",
    "print(serie, '\\n')\n",
    "\n",
    "# Definir indice en la declaración de la serie\n",
    "\n",
    "serie_2 = pd.Series([9,7, -5, 3], index=['a1', 'a2', 'a3', 'a1000'])\n",
    "print('serie_2 delcarado con indice como lista: \\n')\n",
    "print(serie_2, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible seleccionar elementos de la serie usando su etiqueta, de manera identica al manejo de diccionarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se seleccionobja el valor de obj2, cuya etiqueta es 'a1000'\n",
    "serie_2['a1000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera similar, es posible crear un nuevo indice utilizando asignación sobre la sintaxis anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe que el indice 'a4' no existe inicialmente en serie_2\n",
    "serie_2['a4'] = 8\n",
    "serie_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios**\n",
    "\n",
    "* Cree una serie `obj` con los valores `a`, `c` y `e` de `serie`.\n",
    "* Imprima en pantalla aquellos valores de `obj` que sean mayores que 3. *Hint*: recuerde las *queries* de NumPy.\n",
    "* Cree una serie `obj_3` que contenga todos los múltiplos de 3, menores que 100. *Hint*: Haga conversión de tipo de dato a Series, sobre una comprensión de listas.\n",
    "* Imprima en pantalla los elementos de `obj_3` que sean múltiplos de 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las series de `Pandas` soportan operaciones *elemento a elemento*, estas abarcan: filtrado booleano (ejercicio anterior), multiplicación escalar y en general, funciones matemáticas como la suma, logaritmo, exponencial, etc... \n",
    "\n",
    "Las operaciones sobre elementos de una serie son completamente compatibles con las funciones de `Numpy` y preservan los índices de los elementos operados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('\\n Serie 2 duplicada \\n')\n",
    "print(serie_2*2)\n",
    "\n",
    "print('\\n Serie 2 operada por \"exp()\" de Numpy: \\n')\n",
    "print(np.exp(serie_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los índices permiten trabajar con series de manera similar a los diccionarios por medio del operador `in`. Observe que ```in``` consulta sobre las llaves de un diccionario por lo que un objeto tipo ```Series``` puede considerarse una extensión en funcionalidad para el tipo de dato ```dict```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['a3' in serie_2, 'a4' in serie_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede además crear una serie de pandas a partir de un diccionario, donde el conjunto de índices corresponderá al conjunto de campos del diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {'a1': 7, 'a2': 6.5 , 'a3': 5, 'a4':2.1, 'a1000':1 }\n",
    "serie_a = pd.Series(dic)\n",
    "\n",
    "print('\\n serie_a: serie a partir de un diccionario np: \\n')\n",
    "print(serie_a)\n",
    "\n",
    "idx_b = ['a1','a2','b']\n",
    "serie_b = pd.Series(serie_a, index=idx_b)\n",
    "\n",
    "print('\\n serie_b: indice extra, subseleccion serie_a \\n')\n",
    "print(serie_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En `serie_b` se toman los valores `a1` y `a2` de `serie_a` y se agrega una nueva etiqueta `b`, ausente en `dic` y por tanto ausente `serie_a`. La declaración de `serie_b` toma por tanto los valores existentes y correspondientes a cada índice, asignado en valor `NaN` (not a number) a aquellos índices no existentes en el diccionario inicial (`serie_2`). Se aprecia además que la declaración `pd.Series` no diferencia entre un diccionario `NumPy` y una serie de `Pandas`.\n",
    "\n",
    "El término `b` en `serie_b` es un dato ausente (missing data, NA), las funciones `isnull` and `notnull` manejan este tipo de entradas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(serie_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.notnull(serie_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas funciones son métodos de los objetos `Series` y se puede por lo tanto acceder directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Método isnull: \\n')\n",
    "print(serie_b.isnull())\n",
    "print('\\n')\n",
    "print('Método notnull: \\n')\n",
    "print(serie_b.notnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tipo de objetos presenta además la posibilidad de realizar operaciones aritméticas (sobrecarga de operadores +,-,*,...) , preservando el alineamiento de índices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN se comporta como el \"infinito matematico\" \n",
    "\n",
    "# Suma de series, \n",
    "serie_b + serie_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicacion elemento por elemento\n",
    "serie_b*serie_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicacion Escalar\n",
    "serie_a*5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, las series de `Pandas` permiten el etiquetado de variables, lo cual acerca más su manejo al usado en bases de datos y lo enriquece en comparación a `Numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_a.name = 'notas'\n",
    "serie_a.index.name = 'alumnos'\n",
    "print(serie_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame\n",
    "\n",
    "Una serie representa un arreglo unidimensional enriquecido por índices y manejo de valores faltantes. Un DataFrame por su parte, representa una tabla, donde los datos están contenidos\n",
    "en una estructura ordenada, basada en filas y columnas.\n",
    "\n",
    "Un `DataFrame` posee índices para sus columnas y sus filas. Al igual que la series, se puede observar como un diccionario donde a cada llave, se asigna una serie, cada una de las cuales, comparten los mismos índices. Si bien esta noción describe un objeto 2-dimensional, es posible obtener representaciones dimensionalmente superiores mediante una *indexación* adecuada.\n",
    "\n",
    "Existen varias formas de construir un `DataFrame`, una manera conveniente, consiste en definir un diccionario de listas, todas de igual longitud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'ciudad': ['Temuco', 'Temuco', 'Temuco', 'Iquique', 'Iquique', 'Iquique'],\n",
    "'year': [2000, 2001, 2002, 2001, 2002, 2003],\n",
    "'pob': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}\n",
    "\n",
    "frame = pd.DataFrame(data)\n",
    "\n",
    "'''\n",
    "Observe que la representacion de un dataframe se genera por medio de\n",
    "texto html que jupyter renderiza.\n",
    "'''\n",
    "\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La conversión de dato `dict` $\\to$ `DataFrame` es la más intuitiva, sin embargo, existen distintos inputs posibles para construir DataFrames de manera sencilla. Se listan algunos tipos de datos con conversión nativa y como pandas los interpreta.\n",
    "\n",
    "\n",
    "1. Diccionario de Series: Cada serie se transforma en una columna, los indices de las series se transforman en los indices del DataFrame agregando `NaN` donde corresponda.\n",
    "\n",
    "2. Diccionario de diccionarios: Cada diccionario interno pasa a ser una columna, las llaves de cada diccionario interno forman los indices del DataFrame agregando `NaN` donde corresponda.\n",
    "\n",
    "3. Lista de diccionarios (o series): Cada elemento pasa a ser una fila. Las llaves (o indices) pasan a ser las columnas del DataFrame.\n",
    "\n",
    "4. Arreglo de numpy 2D: Se generan etiquetas de manera automática para las filas y columnas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de archivos\n",
    "\n",
    "Si bien es util crear objetos `DataFrame` a partir diccionarios y listas, esto generalmente se reduce a contextos donde se desean *generar* datos. Si bien este proceso es habitual, por lo general se busca *acceder* a datos recopilados en archivos o bases de datos ya existentes. \n",
    "\n",
    "Como ya se vio, Python ofrece metodos nativos para acceder a archivos y levantar conexiones por medio de `open`. El problema de esta función recae en la manera generica de acceso, que conlleva una estructuración posterior del input, para poder acceder posteriormente por medio de constructores como el de la clase `DataFrame`. \n",
    "\n",
    "Pandas ofrece una manera directa de sortear tal complicación por medio de distintos métodos de importación de archivos estructurados. En este contexto, se tiene acceso a métodos como:\n",
    "\n",
    "* `read_csv` que permite cargar un dataset a partir de una ruta de archivo, URL u objeto tipo `'file'`, donde los campos están separados por coma.  \n",
    "\n",
    "* `read_table` que permite la misma funcionalidad que `read_csv` pero sobre archivos separados por tabulación. \n",
    "\n",
    "* `read_clipboard` lee datos guardados en el buffer de copia del sistema operativo (ctr + c).\n",
    "\n",
    "* `read_excel` que permite leer archivos en foramto .xls o .xlsx.\n",
    "\n",
    "* `read_pickle` que permite leer objetos en formato pickle.\n",
    "\n",
    "* `read_sql` que permite leer resultados de queries SQL.\n",
    "\n",
    "Al importar datos, pandas realiza conversión de datos de manera automatica (*type inference*).\n",
    "\n",
    "Una manera sencilla de inspeccionar DataFrames importados es observar una cantidad reducida de elementos usando los métodos `head`, para los primeros elementos y `tail` para los últimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeros 3 elementos, el agumento default es n=5.\n",
    "frame.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultimos 3 elementos\n",
    "frame.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un `DataFrame` hereda el método `.shape` presente en los arreglos de `NumPy`, este sirve para observar el número de filas y columnas de un DataFrame, observe que `.shape[0]` ignora al índice del Dataframe como columna y por tanto no lo contabiliza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observacion: el atributo shape es una tupla.\n",
    "frame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios**\n",
    "\n",
    "1. Utilice la función `pd.read_csv()` sobre el archivo `./data/timesData.csv`. Asigne el resultado de la operación a la variable `df`. ¿Qué tipo de objeto es `df`?\n",
    "\n",
    "2. La base `timesData` consiste en un ranking de universidades a nivel mundial ([link de acceso](https://www.timeshighereducation.com/world-university-rankings)). Observe las primeras 5 filas de tal base.\n",
    "\n",
    "3. Imprima en pantalla el número de observaciones del dataset.\n",
    "\n",
    "4. Imprima el numero de columnas del dataset.\n",
    "\n",
    "5. Dentro de las funcionalidades más comunes al cargar datasets con pandas se encuentran, `'path'` que en el caso de este ejercicio es `'path'=./data/timesData.csv`, `'sep'` que en el caso de `pd.read_csv()` es `'sep' = ','`. Investigue accediendo a la documentación de la función `pd.read_csv()` las funcionalidades de:\n",
    "    1. `'index_col'`\n",
    "    2. `'header'`\n",
    "    3. `'chunksize'`\n",
    "\n",
    "7. Genere un objeto `zip` con dos listas, luego construya un `DataFrame` pasando el objeto `zip` correspondiente como argumento. Escoga una de las listas y genere una enumeración, construya un `DataFrame` con el objeto resultante.\n",
    "\n",
    "6. Es posible importar datos en formato html directo utilizando `pd.read_html()`. Debido al formato de tablas html, pandas puede reconocer de manera relativamente robusta datos tabulados dentro de páginas. Utilice esta función para importar la [tabla histórica de la primer división del futbol chileno](https://es.wikipedia.org/wiki/Anexo:Tablas_hist%C3%B3ricas_de_la_Primera_Divisi%C3%B3n_de_Chile) según la Rssf. Para ello, entregue el link de la pagina correspondiente de wikipedia en formato sting como argumento de `pd.read_html()`. Observe que el objeto generado será una lista de tablas convertidas a DataFrame de manera nativa por pandas, seleccione la tabla correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera contrapuesta a la carga de dataset, se tiene el guardado de datos. Por lo general se realiza para consolidar un procesamiento inicial (transformaciones y limpieza). \n",
    "\n",
    "En este contexto pandas ofrece métodos de almacenamiento en distintos formatos, así, si `df` es un objeto tipo `DataFrame`, es posible almacenarlo según:\n",
    "\n",
    "* `df.to_csv()`: guarda en formato csv, es el más sencillo pues representa los datos como archivos de texto separados por coma.\n",
    "\n",
    "* `df.to_pickle()`: guarda un dataset directamente en formato pickle.\n",
    "\n",
    "* `df.to_feather()`: guarda un dataset en formato feather. Este, es un formato binario, rápido, liviano y de fácil uso. Variados benchmarks lo califican como el formato más eficiente den terminos de velocidad y compresión.\n",
    "\n",
    "En cada uno de estos métodos, se debe proporcionar una ruta de almacenamiento mediante el parámetro string `path_or_buf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios** \n",
    "\n",
    "1. Guarde el csv ubicado en `'./data/timesData.csv'` en formato `feather`.\n",
    "\n",
    "2. Los comandos mágicos de IPython (consola interactiva de Python) son funcionalidades que permiten controlar el comportamiento de la consola IPhyton. Se puede acceder a un listado de estas funcionalidades, junto con su correspondiente documentación, por medio de:\n",
    "```python\n",
    "%magic\n",
    "```\n",
    "Utilice el comando `%%timeit` para medir el tiempo de carga del mismo dataset en los formatos `csv` y `feather`. ¿Cual es más rápido en la carga?\n",
    "\n",
    "3. Utilice el método `.info()` de la clase `DataFrame` para inspeccionar el uso de memoria al cargar un dataset en los formatos  `csv` y `feather`.¿Espera alguna diferencia de peso? \n",
    "\n",
    "4. El formato HDF5 permite el almacenamiento de múltiples arreglos en grandes cantidades. Posee interfaces para lenguajes como Java, Julia, MATLAB, R y Python. Por sus caracteristicas, se recomienda su uso en datasets que no caben en la memoria del equipo de trabajo. Pandas ofrece la clase `HDFStore` con un constructor del tipo `pd.HDFStore(path)` donde `path` es una ruta en la cual se genera un archivo sobre el cual se guardan arreglos. Un objeto tipo `HDFStore` puede almacenar dataframes usdando notación de diccionario, Así, si `hdf` es tipo `HDFStore` y `df` es tipo `DataFrame` es posible guardar y acceder a la información de `df` por medio de `hdf['df_data'] = df` donde `df_data` es un nombre arbitrario. \n",
    "    1. Genere una serie `ser` de pandas. Inicalice un constructor `HDFStore` en modo 'w+' asignadolo a la varaible `hdf`. Almacene la serie `ser` en el objeto `hdf` usando la llave 'serie'.\n",
    "    2. Almacene un dataframe `df` que contenga la información de `'./data/timesData.csv'` en la llave 'times_df'. Para ello tendrá que convertir la columna 'university_name' al tipo 'str' usando el método `.astype('str')` reemplazando a la columna orifinal por medio de:\n",
    "    ```python\n",
    "    df['university_name'] = df['university_name'].astype('str')\n",
    "    ```\n",
    "¿Qué ocurre si no hace tal conversión de tipo de dato?\n",
    "\n",
    "**Obs**: Para el trabajo de estos ejercicios puede ser necesaria la instalación de los paquetes `pyarrow` y `tables`.\n",
    "\n",
    "**Obs**: Los archivos tipo feather se guardan con extensión `.feather`. Por otra parte, los archivos tipo HDF5 usan la extensión `.h5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexación\n",
    "\n",
    "Las clases `Series` y (en especial) `DataFrame` poseen una estructura bastante flexible en cuanto al acceso y etiquetado de datos. Para comprender las funcionalidades ofrecidas, se comienza con estudiar los objetos de tipo `Index`. \n",
    "\n",
    "En pandas, los objetos tipo `Index` dan soporte a los ejes (filas, columnas), permitiendo etiquetarlos, además de proporcionar metadata útil. Los objetos tipo `Index` pueden ser obtenidos mediante conversión de tipo de datos sobre listas.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se crea una serie de pandas y se estudia su indexado con detenimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie = pd.Series(range(10), index = list('abcdefhijk'))\n",
    "idx = serie.index\n",
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar, el constructor de objetos `Series` acepta como argumento una lista contenedora de etiquetas para cada fila de la serie. Por otra parte, `idx` es de tipo `Index`, este presenta métodos de acceso similares a las listas pero es *inmutable*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start:stop:step\n",
    "idx[:6:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios**\n",
    "\n",
    "1. Compruebe que `idx` es inmutable. ¿Qué ventaja tiene esta inmutabilidad?\n",
    "\n",
    "2. Compruebe el valor de verdad de la siguiente afirmación: \n",
    "    \"*Los indices de pandas se comportan como conjutos de  Python, a diferencia de estos, permiten elementos repetidos*\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se estudió anteriormente, la indexación en `Series` funciona de manera similar a arreglos de NumPy y a Diccionarios. Una funcionalidad extra que poseen estos objetos es la **reindexación**. Esta consiste en reorganizar y crear datos basandonde en la idexación.\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "En la serie `serie_a` estudiada anteriormente, se puede ver le efecto de la reindexación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se redefine la serie\n",
    "dic = {'a1': 7, 'a2': 6.5 , 'a3': 5, 'a4':2.1, 'a1000':1 }\n",
    "serie_a = pd.Series(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una nueva serie reindexando la anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_b = serie_a.reindex(['a1','a1000','a2','a3','a4'])\n",
    "serie_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo cual reordena la inforación según la lista de indices proporcionada. Por otra parte, es posible añadir indices nuevos a tal lista, en cuyo caso se agregan valores nulos a la serie inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_b = serie_a.reindex(['a1','a1000','a2','a2000','a3','a3000','a4','a4000'])\n",
    "serie_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio** \n",
    "\n",
    "1. Estudie el método `.fillna()` sobre la serie `serie_b`. Utilice el argumento `'methdod'` proband con los valores `ffill` y `bfill`. ¿Qué hacen estos métodos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El reindexado de un `DataFrame` tiene un grado más de libertad, pues permite alterar indices y columnas. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se estudia el dataframe `frame` antes definido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se redefine frame\n",
    "data = {'ciudad': ['Temuco', 'Temuco', 'Temuco', 'Iquique', 'Iquique', 'Iquique'],\n",
    "'year': [2000, 2001, 2002, 2001, 2002, 2003],\n",
    "'pob': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}\n",
    "\n",
    "frame = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se reindexan las filas, eligiendo los indices 2 y 5. Simultaneamente se cambia el orden de estos indices, mostrando 5 y luego 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_b = frame.reindex([5,2])\n",
    "frame_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para reindexar comulnas, se utiliza el argumento 'columns'. En ese caso se eligen las columnas 'pob' y 'ciudad'. Si se agrega tanto un indice como una columna inexistente al reindexado, se llenará con valores `NaN` donde corresponda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_b = frame.reindex([5,2], columns=['pob', 'ciudad'])\n",
    "frame_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La gran utilidad del indexado de series y dataframes pasa por el filtrado y sub selección de datos. En el caso de las series, se sabe que la selección de indices actua de manera análoga a los diccionarios. Esto es útil para acceder a registros particulares, sin embargo, para operaciones con una alta cantidad de datos, se hace conveniente el uso de *slicing*, caracteristica que heredan de NumPy.\n",
    "\n",
    "Es posible seleccionar una *slice* sobre los indices de una serie utilizando la nomenclatura ya conicida, pero cmabiando números por índices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se estudia la serie_b\n",
    "serie_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se hace una slice entre los indices `a2000` y `a4`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_b['a2000':'a4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma manera se puede acceder con indexado posicional (números enteros en lugar de indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se selecciona el segundo elemento de serie_b\n",
    "serie_b[1] == serie_b['a1000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se repite el slice `'a2000':'a4'` usando indexación posicional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_b[['a2000','a3000','a4000']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_b[3:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Aparte de poder elegir slices de datos, es posible seleccionar un conjunto determinado de elementos entregando una lista como argumento al indexador `[]`. En una linea de código, haga que los valores de 'a2000','a3000' y 'a4000' en `serie_b` sean -1. *Hint*: El operador '=' esta sobrecargado en los objetos `Series` y `DataFrame` de manera que asignar un único valor un subconjunto de filas de tales objetos es válido. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto dataframes, el acceso es similar al de las series. En este caso, las columnas son accedidas de manera directa al utilizar `[]` (en las series son los índices). \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se accede a los datos ubicados en `'data/timesData.csv'` y se observan las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/timesData.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aprecia que las columnas de un `DataFrame` son un objeto tipo `Index`. Se debe tener en cuenta:\n",
    "\n",
    "1. Al utilizar una lista de elementos de la forma `df[lista]`, donde `df` es un objeto `DataFrame`, pandas reconoce la operación como indexación **sobre columnas**.\n",
    "\n",
    "2. Al utilizar slices númericos de la forma `start:stop:step` y filtrados booleanos (o *queries*), pandas reconoce la operación como indexación **sobre filas**. \n",
    "\n",
    "Como las series son unidimensionales, los casos 1 y 2 son equivalentes. Por otra parte, en dataframes se puede llegar a confusiones de no tener lo anterior claro. \n",
    "\n",
    "Se procede a realizar operaciones de indexado sobre las columnas de `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccion de columnas por lista\n",
    "col_list = ['world_rank','university_name', 'num_students']\n",
    "df[col_list].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccion de columna única"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing de filas por filtrado booleano e indexación numerica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desde la fila 5 hasta la 20 saltando 3 elementos.\n",
    "df[5:21:3]\n",
    "\n",
    "# Observe que una lista de enteros no funciona: df[[1,5,6,9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrado booleano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# universidades con enseñanza superior a 90 e investigación inferrior a 90\n",
    "df[(df['teaching'] > 90) & (df['research'] < 90)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso anterior, `&` es el operador lógico para 'y' y `|` es el operador lógico para 'o'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede especificar el indexado por medio de los operadores `loc` e `iloc`. Estos permiten seleccionar subconjuntos de filas y columnas utilizando notación tipo NumPy. Así, si se desea acceder a indexado por etiquetas, se utiliza `loc`, mientras que si se desea acceder a indexado posicional, se utiliza `iloc`.\n",
    "\n",
    "La sintaxis en ambos casos es `df.loc[row,col]` para `loc` row y col son etiquetas para indices y columnas, en el caso de `iloc`, row y col con posiciones absolutas en el dataframe (numeros enteros). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:10,'world_rank':'research']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el indexado de `df` es un rago númerico, se puede utilizar `:10` como argumento de `loc`.\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "1. En el dataframe `df` selecciones las universidades en los puestos 1,3,5 y 7. Seleccione además las columnas 'teaching', 'international' ,'research' y 'citations' utilizando `.iloc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se cargan los datos `'data/timesData.csv'` en la variable `df` y se asignan los datos de 'world_rank' como índce. Se ve que pueden existir múltiples índices con el mismo valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/timesData.csv', index_col='world_rank')\n",
    "'''\n",
    "Obs: los valores True se mapean a 1 y False a 1, luego sum \n",
    "cuenta la cantidad de veces que aparece el ínidce 1 en el dataframe\n",
    "'''\n",
    "sum(df.index == '1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "otra forma de verificar si existen índices duplicados, es por medio del atributo `is_unique` de los objetos tipo `Index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para observar el comportamiento de pandas con múltiples índices, seleccionamos el índice '1' en `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexado Jerárquico\n",
    "\n",
    "El manejo de índices duplicados puede atacarse por medio de *indexado jerárquico*. Este consiste la creación de objetos `Index` multi-nivel, que por tanto se pueden aplicar tanto a filas como a columnas de un `DataFrame`. \n",
    "\n",
    "Los índices multi-nivel proporcionan una manera de trabajar con datos de mayor dimensionalidad proyectado en dimensiones menores. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    " La carga de datos por medio de `pd.read_{format}()` donde `{format}` hace referencia a `csv`, `feather`, `pickle`, etc... posee la funcionalidad de cargar datasets con indices multi-nivel. En el caso de `'data/timesData.csv'`, se sabe que si asociamos el índice al ranking de la universidad, esté se repetirá. Observando el dataset más en profundidad, se aprecia que tal recepción (para rankings desde el 0 al 200) se debe a que se condensa información para los años 2011, 2012, 2013, 2014, 2015, 2016. Para solucionar esto se pueden importar 'indices por año', es decir, existe el nivel 'year', sobre el cual aparece el índice 'world_rank' proyectado, esto se hace mediante el comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/timesData.csv', index_col=['year', 'world_rank'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionar por año se hace bastante sencillo con este formato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2011].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se estudia el índice de `df` se aprecia que es un nuevo objeto tipo `MultiIndex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Construya un objeto `DataFrame` consistente en números aleatorios entre 0 y 1. El `DataFrame` debe poseer 5 filas y 4 columnas. \n",
    "\n",
    "    1. El índice debe tener dos niveles: 'a' con sub-valores 1,2 y 'b' con su valores 1,2,3. \n",
    "    2. Las columnas deben poseer dos niveles: 'lvl_1' con sub-valores 1,2 y 'lvl_2' con sub-valores 1,2.\n",
    "\n",
    "*Hint*: Declare el índice/columna como una lista de listas cada una con la misma cantidad de elementos, componente por componente, establezca una relación del tipo 'a' $\\to$ 1, 'a' $\\to$ 2, 'b' $\\to$ 1, ..., 'b' $\\to$ 3. Use los argumentos 'index' y 'columns' en el constructor de `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible ordenar los índices multi-nivel de forma lexicográfica según nivel, esto mejora el rendimiento de consultas sobre índices\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se hace una consulta sobre `df` sin ordenar los valores para el nivel 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tercer lugar en el ranking para el año 2015\n",
    "df.loc[(2015,'3')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al hacer la consulta, se observa la advertencia `PerformanceWarning`, midamos su performance en cuanto a tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df.loc[(2015,'3')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ordenar los índices de un `DataFrame` se utiliza el método `.sort_index()`, se puede seleccionar el nivel a ordenar por medio del parámetro `level`. Por otra parte, el parámetro `inplace` permite asignar el nuevo orden de los índices directamente al `DataFrame` (similar a los métodos de modificación de listas ej, `.append()`). \n",
    "\n",
    "**Obs**: Es una buena práctica utilizar métodos `inplace` cuando sea posible, pues es más eficiente en cuanto al manejo de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index(inplace=True)\n",
    "df.loc[(2015,'3')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ve como la advertencia desaparece. Se mide el rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df.loc[(2015,'3')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver una ligera reducción del tiempo de ejecución. \n",
    "\n",
    "**Obs**: El slicing multidimencional opera con tuplas. Así, si se desea hacer un *slice* sobre `index_1` entre las etiquetas `index_1_lower_bound`  y `index_1_upper_bound` manteniedo `index_0` fijo, puede hacere usando:\n",
    "\n",
    "```python\n",
    "(index_0, index_1_lower_bound) : (index_0,index_1_upper_bound)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio** \n",
    "\n",
    "1. Utilice `.loc` para acceder a los primeros 10 puestos del año 2016, rescatando las columnas entre 'university_name' y 'total_score'. \n",
    "\n",
    "*Hint*: Observe que el índice 'world_rank' está ordenado de manera lexicográfica, por tanto no podrá usar slicing de tuplas. Utilice en su lugar una lista de tuplas generada con comprensión de listas, donde la primera componente es fija. Para la dimensión de las columnas utilice *slicing* normal sobre las etiquetas solicitadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcionalidades de Preparación y Limpieza \n",
    "\n",
    "Durante el análisis de datos, gran parte de los esfuerzos se centran en la preparación de estos. Este proceso conlleva, la carga, limpieza y transformación sobre los datasets asociados a un problema, con tal de poder extraer información de utilidad por medio de herramientas de visualización, humanización y aprendizaje. \n",
    "\n",
    "La finalidad de este capitulo es porporcionar un conjunto de herramientas presentes en Python y Pandas para facilitar el proceso de preparación de datos.\n",
    "\n",
    "### Manejo básico\n",
    "\n",
    "Manejando el acceso a filas y columnas, se hace necesario operar sobre estas, en este contexto, el operador ```del```, nativo de Python, está sobrecargado de manera tal que permite eliminación de columnas, usando la misma sintaxis de eliminación de datos en un diccionario.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se trabaja con el ranking de universidades para el año 2016, se estudian los primeros 30 puestos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se carga el dataframe con multi indice\n",
    "df = pd.read_csv('data/timesData.csv', index_col=['year', 'world_rank'])\n",
    "\n",
    "# Se ordena para mejorar rendimiento en busquedas multi indice\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener el sub-dataset solicitado, se genera una lista de tuplas con las posiciones deseadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = [(2016,str(i)) for i in range(1,31)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se proporciona tal lista al atributo `.loc` de la forma:\n",
    "\n",
    "```python\n",
    "df.loc[positions]\n",
    "```\n",
    "\n",
    "pandas nos muestra una advertencia de deprecación y nos instruye a utilizar el método `.reindex()`. Sin embargo, tal método no puede manejar multi-indices con niveles duplicados. \n",
    "\n",
    "La solución es entregar un objeto `MultiIndex` directamente al atributo `.loc`. Para ello, se procesa la lista de datos generada, haciendo uso de las propiedades de conjunto que lo objetos `Index` y `MultiIndex` poseen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se genera un MultiIndex con las posiciones deseadas\n",
    "positions = df.index.intersection(positions)\n",
    "\n",
    "# Se adquiere el sub-dataframe buscado\n",
    "df_16 = df.loc[positions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para le `DataFrame` nos interes eliminar la comlumna 'income' esto se hace facilmente mediante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_16.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_16['income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por su parte, si se desea eliminar un conjunto de columnas, es posible alamcenar sus nombres en una lista y utilizar el método `.drop()` de un `DataFrame`.\n",
    "\n",
    "Borramos las columnas (axis 1) 'international' y 'citations'. Hacemos tal operación 'inplace' para mejorar la eficiencia.\n",
    "\n",
    "**Obs**: En los objetos `DataFrame` de pandas se hace alición al concepto 'axis', este se refiere a filas (axis = 0) y comlumnas (axis=1). En general se refiere a las dimensiones de un arreglo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_16.drop(['international','citations'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra operación nativa, sobrecargada en objetos `DataFrame` es `len()`. Esta entrega la cantidad de registros asociados a un `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que se heredan las propiedades de los arreglos n-dimensionales de NumPy, se adquiere un comportamiento similar en cuanto al *bradcasting* de arreglos. \n",
    "\n",
    "En pandas esto se denomina *data aligment* y se vio al sumar las series `serie_a` y `serie_b` al comienzo de la lección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_b + serie_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regla general, es que el alineamiento de datos introducirá valores nulos `NaN` en los pares (indice,etiqueta) donde no exista *overlap*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio** \n",
    "\n",
    "1. Genere dos objetos `DataFrame` de `shape` igual a`(4,4)`. Ambos deben poseer 2 indices en común, además de 2 columnas en común. Ejecute operaciones aritméticas del tipo A + B, A - B, A x B. ¿Se puede caracterizar el comportamiento de estas operaciones?\n",
    "\n",
    "2. Si `df_a` y `df_b` son objetos `DataFrame`, la operación `df_a` + `df_b` puede ser expresada por `df_a.add(df_b)`. El método `.add()` en este caso tiene la opción 'fill_value'. Ejecute las operaciones del ejercicio 1 en terminos de métodos de la clase `DataFrame`. ¿Qué utilidad posee el parámetro 'fill_value'?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al momento de operar entre DataFrames y Series, se pueden obtener resultados no esperados. En estos contexto, la indexación juega un papel fundamental:\n",
    "\n",
    "**Ejemplo** \n",
    "\n",
    "Se generan los dataframes `df_ej` y `df_ej2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ej = pd.DataFrame(np.arange(10).reshape(5, 2) +1,columns=list('ab'))\n",
    "df_ej2 = pd.DataFrame(np.arange(18).reshape(6, 3) +1000,columns=list('acb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada uno se muestra en pantalla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('df_ej  : \\n', df_ej, '\\n')\n",
    "print('df_ej2 : \\n', df_ej2, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea la serie `ser1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1 = np.repeat(1000,6)\n",
    "ser1 = pd.Series(ser1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se estudian las siguientes operaciones aritmeticas del tipo `DataFrame` - `Serie`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# los indices de la serie \n",
    "print('Caso 1: Suma DataFrames')\n",
    "print(df_ej + df_ej2,'\\n')\n",
    "\n",
    "print('Caso 2: DataFrame + serie')\n",
    "print(df_ej + ser1,'\\n')\n",
    "\n",
    "# Se traspone el data frame antes de la suma\n",
    "print('Caso 3: DataFrame traspuesto + serie')\n",
    "print(df_ej.T + ser1)\n",
    "\n",
    "# Columnas de un DataFrame traspuesto\n",
    "\n",
    "print('\\n Columnas del DataFrame traspuesto: \\n', df_ej.T.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede deducir que la suma de `Series` y `DataFrame` se comporta como una \"unión externa\" basada en las columnas e índices: \n",
    "\n",
    "* En el caso 1, los DataFrames comparten las columnas `a` y `b` donde los valores se suman uno a uno, se añade la columna `c` (unión externa) con los valores faltantes correspondientes.\n",
    "\n",
    "* En el caso 2, la serie se comporta como un arreglo de `NumPy` en donde se suma de manera \"horizontal\". (Por tanto los índices pasan a ser columnas, trasponer la serie no cambia este hecho)\n",
    "\n",
    "* En el caso 3, las columnas del DataFrame pasan a ser las nuevas columnas, por lo que la suma se comporta como seria de esperar.\n",
    "\n",
    "Si a diferencia del caso 2, se desea sumar a través de las columnas, es posible hacerlo indicando el eje `axis`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ej.add(ser1,axis=0).fillna(value=':)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación de funciones\n",
    "\n",
    "Como ya se mencionó, los objetos de Pandas se comportan de manera similar a arreglos de `NumPy`. En este aspecto, los DataFrames y Series de Pandas, soportan de manera nativa las funciones de *elemento por elemento* :\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se genera el objeto `DataFrame` `df_c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = pd.DataFrame(np.random.randn(5, 3), columns=['Arica','Santiago','Temuco'])\n",
    "df_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen dos funciones `lambda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norma_1 = lambda x: (np.sum(x**2, axis = 0))/len(x)\n",
    "norma_2 = lambda x: np.abs(x).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`norma_1` calcula la media cuadratica un arreglo (norma $l^2$), por su parte, `norma_2` calcula el valor absoluto de un arreglo (el resultado será un arreglo) y adquiere el máximo (norma $l^{\\infty}$). Al aplicar la función `norma_1` de manera usual sobre `df_c`, se observa que la función se aplica sobre las columnas (dejando axis 1 fijo y moviendo axis 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicación \"Clasica\" de la funcion metrica_1\n",
    "print(norma_1(df_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es de esperar por tanto, que aplicar funciones que actuan sobre un elemento (y no sobre arreglos) operen en cada elemento del `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(df_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se desea aplicar una función en las filas (no columnas como fue el comportamiento predeterminado), se hace uso del método `.apply()`, donde se entrega el parámetro 'axis'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Se aplica metrica 1 en la direccion de las filas, deja axis 0 fijo \n",
    "y mueve axis 1\n",
    "''' \n",
    "df_c.apply(norma_1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una buena práctica usar `.apply()` pues permite especificar un eje sobre el cual aplicar una función, sea este por filas o por columnas. \n",
    "\n",
    "Cabe mencionar que se puede forzar una función a operar de manera elemento - elemento (comportamiento de np.abs) en vez de operar sobre arreglos (comportamiento de apply). Para esto se usa el método `.applymap()`. \n",
    "\n",
    "Observemos que `norma_2` esta diseñada para operar sobre arrgelos, si se fuerza a operar de manera elemento - elemento, degrada al valor absoluto de cada elemento de un `DataFrame` o matriz. Se demuestra esta observación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.applymap(metrica_2) - np.abs(df_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los objetos de `Pandas` están provistos de métodos estadísticos básicos.\n",
    "\n",
    "**Ejercicios**\n",
    "\n",
    "1. Explore los métodos `sum`, `cumsum` `mean`, `std`, `idxmin`, `idxmax` y `describe` (especialmente útil).\n",
    "\n",
    "2. Observe que varios de estos métodos poseen el parámetro 'level'. Cargue el dataset de ranking de universidades utilizando multi - indices para 'world_rank' (nivel 1) y 'year' (nivel 0). Calcule las funciones anteriores sobre ambos niveles.\n",
    "\n",
    "3. Explore el método `.corr` del `DataFrame` anterior.\n",
    "\n",
    "**Obs**: Ponga atención a los parámetros 'axis' y 'skipna'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a la preparación y limpieza de datos, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos faltantes\n",
    "\n",
    "La falta de información en bases de datos es común en tareas de análisis de datos, la filosofía de `Pandas` en es este aspecto se basa en facilitar el manejo de datos faltantes lo más posible. En el caso de  datos numéricos, `Pandas` usa la notación `NaN` (not a number) para representar valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_c = serie_a + serie_b\n",
    "serie_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Python, el tipo de dato `None` juega el papel *not aviable* a valor `NA`, de manera análoga, pandas reconoce el objeto `nan` de NumPy como dato `NA`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Estudie los métodos `dropna`, `fillna`, `isnull` y `notnull`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtrado de valores faltantes\n",
    "\n",
    "Existen varias maneras de filtrar datos faltantes. Una forma es hacerlo usando filtrado booleano a través del método `isnull`, otra más directa es usar el método `dropna`. En series por ejemplo, este último método retorna una sub-serie consistente solo de los valores presentes.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se genera una serie con valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan as NA\n",
    "data = pd.Series([2, NA, 4, NA, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrado inderecto de valores faltantes (mlpa práctica pero intuitivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[data.notnull()]) # filtrado logico ---> indirecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Método directo de eliminación de valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.dropna()) #dropna --> método directo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que para el método `.dropna()` se tiene opción 'inplace'.\n",
    "\n",
    "En el caso de DataFrames el proceso cambia. El aumento en la dimensión provoca la complicación y básicamente depende si se desean borrar filas o columnas, y en caso de querer eliminar es necesario tener claro como se desea hacer este proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.DataFrame([[1,2,4], [8,NA,NA],\n",
    "                         [NA,NA,NA], [NA,NA,16],\n",
    "                          [32,NA,64]])\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se limpia `data_set` usando `.dropna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limpio_1 = data_frame.dropna()\n",
    "limpio_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando `how = 'all'` eliminará solo aquellas filas consistentes solo de valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limpio_2 = data_frame.dropna(how='all')\n",
    "limpio_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Elimine de `data_frame` aquellas columnas consistentes únicamente de valores faltantes. *Hint*: utilice how y axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra manera de atacar el problema de datos faltantes es completando ciertos valores faltantes en vez de filtrarlos. En `Pandas` se puede usar el método `fillna`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios**\n",
    "\n",
    "1. Genere un DataFrame aleatorio de `10x10` consistente de números aleatorios. Llene los primeros `5` elementos de la primera columna con `NA`. Haga lo mismo con los 3 primeros elementos de la segunda fila. (Hint: `iloc` + `randn` )\n",
    "\n",
    "2. Investigue el argumento `method = ffill` del método `fill_na`.\n",
    "\n",
    "3. Seleccione la tercera fila del DataFrame y reemplace los valores faltantes con el promedio de los demás valores de la fila.\n",
    "\n",
    "4. Reemplace los valores faltantes de la primera columna con el valor 10, los de la segunda columna con el valor 100 y los de la tercera columna con el el valor 1000. Para ello use la función `fillna` y proporcione un diccionario como input del método."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como contraparte a los valores faltantes se encuentran los valores duplicados. En este aspecto el método `duplicated` proporciona un serie booleana indicando que serie esta duplicada. Por su parte, `drop_duplicates` corresponde al símil de `drop_na` para `isnull`.\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "1. Utilice `duplicated` y `drop_duplicates` sobre `data_dup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dup = pd.DataFrame({'A1':['uno', 'dos'] * 3 + ['dos'],\n",
    "                               'A2':[1,1,2,3,3,4,4]})\n",
    "data_dup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a **transformación de datos**, se pueden aplicar técnicas de mapping. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se crea el siguiente objeto `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map = pd.DataFrame({'receta':['huevos', 'harina', 'leche','manjar'],\n",
    "                                  'cantidad':[2,1,1,250]})\n",
    "data_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agrega una columna basada en los valores del DataFrame (mapping explicito):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_vegano = {'huevos':'no', 'harina':'si', 'leche':'no','manjar':'no'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "el diccionario anterio define una aplicación de la forma `key` $\\to$ `value`. \n",
    "\n",
    "El método `.map()` acepta un objeto del tipo `dict` que contiene una relación para los elementos de la serie que opera.\n",
    "\n",
    "**Obs**: Hacer mapping explicito es *case sensitive*.\n",
    "\n",
    "Se crea una nueva columna basada en la información del mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map['es vegano ?'] = data_frame_mapping['receta'].map(es_vegano)\n",
    "data_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, `map` puede recibir una función como elemento y la opera sobre cada elemento de la serie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map['es vegano ?'].map(lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un caso especial de remplazo/mapping es el método `replace` este es más general que `fillna` pues permite remplazar valores a elección de una serie.\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "* Reemplace en la siguiente serie los valores 'error' y 'Error' por NA y 100 respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie = pd.Series([1, 'error', 2, 'error', -1000., 3, 'Error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tienen a disposición métodos de **discretización** y **binning**. Estos permiten separar datos numerocos en secciones o *bins*. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se considera la siguiente serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income = [98,2, 28,11,53,61,33,17,19,40,78,8,3,13,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se desea dividir tal serie en bins de 0 a 30, 31 a 50, 51 a 70, 71 a 90  y 91 al 'infinito'. Para ello se usa el método `.cut()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0,30,50,70,90, np.infty]\n",
    "secciones = pd.cut(income,bins)\n",
    "secciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan los códigos o categorias asociadas a cada *bin*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secciones.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la función `pd.value_counts()` permite contar frecuencias (preferentemente) e variables categoricas. Esto permite obtener el histograma inducido por los *bins* propuestos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(secciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a la **detección y manejo de outliers** se puede hacer uso de operaciones sobre arreglos.\n",
    "\n",
    "**Ejercicios**\n",
    "\n",
    "1. Genere un DataFrame de 4 columnas con 1000 observaciones. Utilice `describe` para estudiar las características de su DataFrame.\n",
    "\n",
    "2. Seleccione aquellos valores del dataframe mayores que 1.5 y menores que -1,5 y reemplace esos valores por 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien es posible **permutar** índices. En aplicaciones de análisis de datos, se requiere hacer permutaciones aleatorias en los datos para realizar por ejemplo operación de *data augmentation*. Esto se puede hacer fácilmente usando la función `permutation` del módulo `random`, donde tal función se debe llamar usando como argumento la longitud del eje `axis` sobre el cual se desea permutar.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se genera un objeto `DataFrame` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_perm = pd.DataFrame(np.arange(10*10).reshape((10,10)))\n",
    "data_perm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se smplea una permutación de los índices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = np.random.permutation(10)\n",
    "sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `.take()` proporciona por defecto las filas del DataFrame tomadas de un arreglo (equivalente al atributo `loc`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_perm.loc[sampler,:]\n",
    "data_perm.take(sampler) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para seleccionar un subconjunto sin reemplazo, se puede usar directamente el método `.sample()`. Se muestran 5 filas de `data_perm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_perm.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs**: Para seleccionar con reemplazo existe la opción `replace = True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro tipo de transformación es el manejo de variables `Dummy`o indicadores. Estas consisten en 'códigos' que representan a través de números los distintos valores que puede tomar una variable categórica de interés.\n",
    "\n",
    "Si un DataFrame tiene `k` distintos valores en una columna, se puede obtener una matriz indicadora con `k` columnas consistentes de 1's y 0's y usar aquellos valores como variables dummy para representar tal columna.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "En pandas la función `get_dummies` genera tal matriz indicadora de manera sencilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummy = pd.DataFrame({'llave_1':['a','a','b','c','b','a'],\n",
    "                            'data_1':range(6)})\n",
    "data_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene la matriz indicadora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(data_dummy['llave_1'])\n",
    "dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera un nuevo `DataFrame` concatenando el dataframe inicial con la matriz indicadora construida, para ello se usa el método `.join()`. Se mantiene la columna 'llave_1' para mostrar el funcionamiento de la matriz `dummies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummy_mod = data_dummy[['data_1','llave_1']].join(dummies)\n",
    "data_dummy_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios**\n",
    "\n",
    "El objetivo de esta lista de ejercicios es ejercitar algunas de las técnicas que pueden ser usadas para rellenar datos faltantes por medio de un caso de estudio. \n",
    "\n",
    "El caso de estudio se centra en un conjunto de datos de la siguiente publicación\n",
    "\n",
    "- \\[1\\] [Zhang, S., Guo, B., Dong, A., He, J., Xu, Z. and Chen, S.X. (2017) Cautionary Tales on Air-Quality Improvement in Beijing. Proceedings of the Royal Society A, Volume 473, No. 2205, Pages 20170457.](https://royalsocietypublishing.org/doi/pdf/10.1098/rspa.2017.0457)\n",
    "\n",
    "Disponible en el repositorio de [UCI](https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data).\n",
    "\n",
    "Las columnas incluidas en este conjunto de datos son las siguientes:\n",
    "\n",
    "- No: número de fila\n",
    "- year: año de datos en esta fila\n",
    "- month: mes de datos en esta fila\n",
    "- day: día de datos en esta fila\n",
    "- hour: hora de datos en esta fila\n",
    "- PM2.5: concentración de PM2.5 (ug / m ^ 3)\n",
    "- PM10: concentración de PM10 (ug / m ^ 3)\n",
    "- SO2: concentración de SO2 (ug / m ^ 3)\n",
    "- NO2: concentración de NO2 (ug / m ^ 3)\n",
    "- CO: concentración de CO (ug / m ^ 3)\n",
    "- O3: concentración de O3 (ug / m ^ 3)\n",
    "- TEMP: temperatura (grado Celsius)\n",
    "- PRES: presión (hPa)\n",
    "- DEWP: temperatura de rocío (grados Celsius) - temperatura de rocío es la más alta temperatura a la que empieza a condensarse el vapor de agua contenido en el aire.\n",
    "- RAIN: precipitación (mm)\n",
    "- wd: dirección del viento\n",
    "- WSPM: velocidad del viento (m / s)\n",
    "- station: nombre del sitio de monitoreo de la calidad del aire\n",
    "\n",
    "Las estaciones están todas ubicadas en el área metropolitana de Beijing. En la siguiente figura los puntos violetas muestran su ubicación, los triángulos son estaciones meteorológicas y los puntos rojos son otras estaciones no incluidas en el conjunto de datos. Las lineas azul y naranjas dividen a la metropolis en zona norte, centro y sur.     \n",
    "\n",
    "<img src=\"figures/beijing.jpg\" style='width:600px'>\n",
    "<center><b>Figura 1:</b> Ubicación de las estaciones de monitoreo - Fuente [1].</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar cargamos el conjunto de datos en memoria. En la ruta `'S4/data/bei'` se encentran 12 archivos `.csv`, cada csv contiene las series de tiempo con frecuencia horaria de las 12 estaciones mencionadas, para 4 años calendario, desde marzo 2013 hasta febrero 2017. Por este motivo, todos los archivos tienen la misma cantidad de filas y columnas.\n",
    "\n",
    "1. Escoga uno de los 12 archivos e importelo en la variable `df`.\n",
    "2. Cuente el número de valores nulos de `df`. *Hint*: Componga `.sum()` con `.isnull()`. \n",
    "3. Genere una serie cuyos índices serán las columnas de `df` y sus valores serán el porcentaje de valores faltantes para cada columna en `df`.\n",
    "\n",
    "Además de contar la cantidad de nulos, dado que se trata de series de tiempo, nos interesa saber la cantidad máxima de valores nulos contiguos.\n",
    "\n",
    "3. Cree una función que encuentre el número máximo de valores nulos contiguos.\n",
    "\n",
    "4. Para las columnas en las que el porcentaje de nulos es menor al 1%, llene los valores faltantes usando `fillna`.\n",
    "    1. Elija  una de tales columnas y llene los valores nulos con 0.\n",
    "    2. Elija una de tales columnas y llene con la moda. \n",
    "    3. Elija una de tales columnas (numérica) y llene con la mediana. \n",
    "    4. Elija una de tales columnas (numérica) y llene con el promedio. \n",
    "    5. Elija una de tales columnas y llene con el método 'ffill' o ''bfill'.\n",
    "  \n",
    "5. (Opcional - Difícil) La librería `glob` permite listar archivos en una ruta proporcionada por el usuario. Utilice `glob.glob('data/bei/*')` para generar una lista de los datasets correspondientes al problema. \n",
    "    1. Recorra tales rutas con un ciclo para generar un gran dataset con toda la información. \n",
    "    2. Utilice indexado jerárquico sobre las columnas para agrupar la información por estación. \n",
    "    3. Repita los análisis anteriores para todas las estaciones. \n",
    "    \n",
    "*Hint*: Estudie el método `.concat()`, puede ser de utilidad generar un diccionario vacío y poblarlo al iterar para cada ruta.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
